---
title: "Integración de datos"
description: |
  Métodos de aprendizaje automatizado como un enfoque para la integración de datos.
author:
  - name: Julián Equihua
    url: https://taller-cuenta-condicion.netlify.app/posts/2022-08-27-integracin-de-datos/
date: 2022-08-27
output:
  distill::distill_article:
    self_contained: false
---

# Aprendizaje de máquina (Machine learning)

Métodos computacionales para aprender de datos con el fin de replicar una tarea o toma de decisión.

Existen varios paradigmas en aprendizaje de máquina, con distintos objetivos:

* El aprendizaje supervisado procura predecir o estimar una variable respuesta a partir de datos de entrada
* El apprendizaje no supervisado procura describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir
* El aprendizaje semi-supervisado procura utilizar datos asociados a una variable respuesta al mismo tiempo que datos sin la variable para procurar mejor estimarla. Estos es intenta explotar datos sin etiquetas para mejorar la estimación.
* El aprendizaje por refuerzo intenta estimar la mejor política posible en un problema de toma de decisiones secuenciales.

Ejemplos de tareas de aprendizaje:

* Predecir si un individuo va a sobrevivir el hundimiento del Titanic,
* Detectar llamados de murciélagos en grabaciones ultrasónicas,
* Clasificar especies de animales en imágenes digitales producidas a través de foto trampeo,
* Estimar el ingreso mensual de un hogar a partir de las características de la vivienda,
* Dividir a los clientes de Netflix según sus gustos.
* Recomendar artículos a clientes de un programa de lealtad o servicio online.

Las razones usuales para intentar resolver estos problemas computacionalmente son diversas:

Quisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente exactitud y precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso. 

```{r, echo = FALSE, fig.align = 'center', out.width = '80%'}

knitr::include_graphics("1_license.jpeg")
```

Igual ver cada fotografía para detectar e identificar los animales ahí presentes toma muchísimo tiempo.

```{r, echo = FALSE, fig.align = 'center', out.width = '80%'}

knitr::include_graphics("2_trampa.jpg")
```

Quisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.

¿Lo mismo para la mejor toma de decisiones en cuanto al manejo de recursos naturales?

También es de gran interés entender de manera más completa y sistemática el comportamiento de un fenómeno, identificando variables o patrones importantes.

¿Dado un conjunto de datos ecológicos podemos hacer generar modelos que nos permitan ir explorando distintos aspectos de la integridad ecosistémica para generar conocimiento?

De una vez adelantamos que el acercamiento que se sigue para generar el índice de integridad
propuesto es uno de aprendizaje supervisado. Existe una variable que sirve de evidencia para el nivel de condición de los distintos ecosistemas del país y a la par se tienen variables que permiten estimar y, eventualmente, explicar este nivel de condición. El único detalle importante aquí es que la evidencia es categórica, y el índice deseado es uno continuo entre 0 y 1. 

El aprendizaje de máquina generalmente se lleva a cabo con ayuda de software especializado. Programas propietarios o de código abierto o comunmente utilizando lenguajes de programación. Por ejemplo R, python y Julia. 

Recomendamos ampliamente aprender los tres en la medida de lo posible pues ofrecen distintas posibilidades y ventajas. En esta sesión trabajaremos con R.

Para comenzar carguemos unos datos ejemplo al espacio de trabajo de R.

```{r, echo = TRUE, fig.align = 'center'}

# Cargar paquetes.
library("raster")

# Cargar un geotiff a R, de hecho el que sirve como evidencia para el modelo de IE.
ruta_evidencia <- "./delta_vp/hemerobia_250m.tif"
raster_evidencia <- raster(ruta_evidencia)

raster_evidencia

```

Para aquellos que no han experimentado con lenguajes de programación para manejo y análisis de datos, acá están pasando varias cosas. Repasamos algunas de ellas.

Primero que nada, un elemento fundamental que debemos mencionar es el de una función. Una función recibe argumentos y lleva a cabo una cierta tarea (aunque suene un poco raro también puede recibir ningún argumento y llevar a cabo una tarea). Las funciones siempre tienen un nombre y los argumentos que reciben se colocan dentro de paréntesis.

```{r, echo = FALSE, fig.align = 'center', out.width = '100%'}

knitr::include_graphics("3_funciones.png")
```

Por ejemplo la función library() lleva a cabo la tarea de cargar un determinado paquete de R. Un argumento que DEBE recibir es el nombre del paquete a cargar. La función raster carga un raster en el espacio de trabajo de R (en la memoria RAM) y un argumento que DEBE recibir es la ruta en el disco duro del raster que se quiere cargar.

Cuando una función carga algo en memoria, esto se debe asignar a un objeto. Es decir que se debe de elegir un "nombre" con el cuál referirse a lo que se está cargando. Se puede elegir cualquier nombre pero se recomienda usar unos juiciosos para recordar de qué se tratan. Para asignar en R comúnmente se usa una flechita pero también puede usarse el símbolo de igual como en otros lenguajes de programación.

```{r, echo = FALSE, fig.align = 'center', out.width = '100%'}

knitr::include_graphics("4_asigobjeto.png")
```

Hemos llamado 'raster_evidencia' al que acabamos de cargar, con este objeto podemos ver una versión cruda del mismo dentro de R.


```{r, echo = TRUE, fig.align = 'center'}

plot(raster_evidencia)

```

Ahora bien, un raster es una estructura que puede resultar muy útil para ciertos propósitos, pero para otros no tanto. Por ejemplo para llevar a cabo un proceso de análisis exploratorio de datos, el formato más común es el de una tabla de datos. De hecho R tiene un estándar de datos ampliamente aceptado llamado [tidy data](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html#:~:text=Tidy%20data%20is%20a%20standard,Every%20row%20is%20an%20observation.) donde se analiza y describe cómo deberían de ser las tablas de datos para análisis. Dejando eso de lado, generalmente existen funciones en R (y en los otros lenguajes) para pasar de un tipo de objeto (dato) a otro. Por ejemplo de un raster a una tabla de datos: 

```{r, echo = TRUE, fig.align = 'center'}

tabla_evidencia <- as.data.frame(rasterToPoints(raster_evidencia))

head(tabla_evidencia)

```

Acá cada fila es una observación (un píxel) y cada columna una variable coordenada x, coordenada y, el valor de hemerobia del píxel. 

```{r, echo = TRUE, fig.align = 'center'}

library("ggplot2")

ggplot(tabla_evidencia, aes(hemerobia_250m, fill = cut(hemerobia_250m, 18))) +
  geom_histogram(show.legend = FALSE) +
  scale_fill_brewer(palette="RdYlGn", direction = -1)

```
