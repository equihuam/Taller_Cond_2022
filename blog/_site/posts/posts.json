[
  {
    "path": "posts/2022-08-29-interoperabilidad/",
    "title": "Interoperabilidad",
    "description": "La interoperabilidad es fundamental para la operación intersectorial e incluso\ninterdisciplinar.",
    "author": [
      {
        "name": "Miguel Equihua Zamora",
        "url": {}
      }
    ],
    "date": "2022-08-31",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nLa multiplicidad de descriptores del entorno natural hacen necesario considerar enfoque que permitan la interoperabilidad y favorezcan la compartición de conceptos fundamentales, como los derivados del sistema de contabilidad ambiental. En este casoel índice de condición: integridad ecosistémica basado en el modelo de 3 capas.\r\n\r\nEl participante conoce la posibilidad de utilizar una aproximación probabilística basada en redes bayesianas para relacionar distintos índices de condición.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-29-interoperabilidad/../../figuras/interoperabilidad.png",
    "last_modified": "2022-09-13T22:40:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-31-flujos-de-trabajo/",
    "title": "Flujos de trabajo",
    "description": "¿Por qué conviene hacer pipelines?.",
    "author": [
      {
        "name": "Julián Equihua Benítez",
        "url": {}
      }
    ],
    "date": "2022-08-31",
    "categories": [],
    "contents": "\r\nPipeline\r\n¿Qué es un pipeline?\r\nEs un conjunto de elementos de procesamiento que se ejecutan en serie. Esto es que la salida de uno de los elementos es la entrada de otro. La infraestructura de automatización de un pipeline cuenta con procedimientos de seguimiento automático de la secuencia de tareas. También tiene la capacidad de detectar cambios y activar automáticamente los procesos necesarios para procesar esos cambios de acuerdo con la secuencia de cómputo prevista.\r\nVeamos un esquema sencillo correspondiente a un pipeline para ajustar un modelo de IE.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"1_pipeline.png\")\r\n\r\n\r\n\r\nEn las sesiones anteriores visto algunos de los pasos que llevaría a cabo un pipeline de este tipo.\r\nTodo se ha ejecutado a mano pero se mostró:\r\nCómo cargar el conjunto de rasters correspondientes a la variable explicativa en memoria e introducirlos a un raster multibanda.\r\nEl raster multibanda anterior se debe transformar a una tabla de datos para poder llevar a cabo cualquier proceso de analítica.\r\nEn el caso del proceso de validación estadística, la tabla se separa en múltiples subconjuntos que iterativamente son utilizados para ajustar modelos predictivos (redes bayesianas en nuestro caso).\r\nCada modelo predictivo parametrizado en el proceso anterior se utiliza para predecir sobre el subconjunto de datos que no fue utilizado en su proceso de ajuste.\r\nEl modelo final resulta de ponderar todos los ajustes ensayados y así se obtiene el ajuste que aplica a la tabla de datos completa.\r\nLuego el modelo final se utiliza para predecir y esta predicción se vuelve el mapa de integridad ecosistémica.\r\n¿Por qué construir un pipeline?\r\nControl\r\nReproducibilidad\r\nEficiencia\r\nEscalabilidad\r\nControl\r\nUn pipeline bien organizado hace que el proceso de modelado sea más flexible. En cierto sentido es como tener un buen diagrama de una máquina donde puedes experimentar e incluso reemplazar partes.\r\nSuena un poco evidente, pero si los procesos de carga y transformación de datos ya funcionan bien, pero el proceso de validación es muy lento, se pueden enfocar los esfuerzos en desarrollarlo de mejor manera. Se debe entender un pipeline justo como parte de una maquinaria, es algo que nos permite encadenar una seria de procesos para llegar a un objetivo u objetivos sin intervención humana. Esto nos lleva a nuestro siguiente punto.\r\nReproducibilidad\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"3_crisis.jpg\")\r\n\r\n\r\n\r\nfuente\r\nNo es un gran secreto que hay una crisis de reproducibilidad en la ciencia.\r\nHay muchas causas para tal crisis pero una de las que se han hecho evidentes recientemente es la incapacidad de reproducir los resultados por cuestiones de administración, procesamiento y análisis de datos.\r\nLo primero que se viene a la mente es no tener acceso al software para reproducir los resultados, o por usar software propietario y carecer de licencia o porque los autores no compartieron su código. Esto nos lleva a hacer un llamado a utilizar software de código abierto y siempre compartir el código con el que se llevó a cabo un proyecto. Por supuesto esto puede ser más o menos delicado por cuestiones de seguridad.\r\nLo segundo que viene a la mente es con qué herramientas se llevan a cabo los procesos. A pesar de vivir en la época del Big data, si somos honestos, casi nadie lleva a cabo sus procesos analíticos con Big Data. La mayoría de los procesos analíticos se llevan a cabo sobre archivos de texto y basicamente lo que podamos acomodar en un documento Excel.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"4_genes.png\")\r\n\r\n\r\n\r\nfuente\r\nA lo mejor es un poco injusto hacer un llamado total a dejar de usar MS Excel o más bien en general las llamadas hojas de cálculo. Por supuesto se trata de una herramienta poderosa en sí misma y muy cómoda para llevar a cabo muchas tareas de gestión y exploraciones de datos. Pero una vez que lo que se quiere hacer queda claro, definitivamente conviene automatizarlo con algún proceso en, por ejemplo R, y organizarlo como un pipeline.\r\nEl artículo de Nature antes citado comienza diciendo que más del 70% de los investigadores encuestados (~1500) fallaron al intentar repetir los experimentos de otro científico. Más del 50% fallaron al reproducir sus propios resultados experimentales.\r\nPor supuesto esto está exacerbado en ciencia que depende de métodos empíricos pero la investigación basada en analítica está lejos de estar a salvo.\r\nSi unos resultados analíticos se obtienen utilizando un pipeline, hay una absoluta certeza de que los resultados son reproducibles. Esto es, con base en una misma entrada, lo que al final arroja un pipeline siempre será lo mismo.\r\nEficiencia\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"5_automate.png\")\r\n\r\n\r\n\r\nPor supuesto que un proceso automatizado de manejo de datos es más veloz que copiar y pegar celdas a mano en una hoja de cálculo tipo Excel. Como ya se mencionó, puede ser útil llevar a cabo procesos con datos en Excel para entender cómo deben ser los pasos y para validar ciertas acciones, pero al final del día lo más confiable y rápido será reproducir eso con un proceso automatizado.\r\nEscalabilidad\r\nCon el advenimiento del Big data, se desarrollan diversas herramientas para la administración, consulta y análisis de datos. Tanto en hardware como en software. Una cosa que se vuelve muy importante es el uso de pipelines para siquiera poder procesar Big Data. Un pipeline no sólo deja controlar el proceso analítico en sí, sino la gestión de los recursos destinados a los procesos y análisis que se están ejecutando. El software para hacer piplines está pensado para poder controlar a qué lugar mandar un cierto proceso (por ejemplo un Nodo en un cluster de máquinas de alto desempeño), determinar la memoria RAM necesaria para cada proceso y organizar todo de tal manera que ningún proceso comience si no se ha terminado uno anterior que es necesario. Todo bajo un constante monitoreo para poder revisar que las cosas van bien y que, de haber errores, estos se están documentando. Naturalmente hay pipelines enormes y pipelines chicas. Algo interesante también es que no se necesita un proyecto gigantesto para aprovechar una plétora de computadoras, poder ejecutar numerosos procesos simultáneamente cuando ya se tiene una pipeline definida es una opción, sin duda, útil.\r\n¿Qué es un contenedor?\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"6_containers.png\")\r\n\r\n\r\n\r\nUn contenedor es una unidad estándar de software que empaqueta código y todas sus dependencias para que una aplicación corra confiablemente sin importar en qué ambiente se esté ejecutando.\r\nHay varios contenedores, los más interesantes para nosotros son:\r\nDocker, por su popularidad y flexibilidad\r\nhttps://apptainer.org/, por estar especializado en procesos analíticos\r\nEstos contenedores permiten generar paquetitos que sencillamente se pueden compartir, listos para usarse en otro ambiente. Para proyectos chicos, una pipeline completa puede vivir en un solo contenedor. Para proyectos muy grandes, módulos importantes de la pipeline pueden vivir en contenedores diferentes y por tanto deben ser orquestados mediante software adicional.¡Hay opciones y son muchísimas!\r\nSoftware\r\nHay muchísimo software para hacer pipelines. Cada lenguaje de programación tiene varias opciones y hay varios orientados a infraestructura particular, por ejemplo, Sagemaker es específicamente para llevar a cabo machine learning en amazon web services.\r\nEn R, uno de los paquetes más populares para hacer pipelines es Drake.\r\n\r\n\r\nShow code\r\n\r\nlibrary(drake)\r\nlibrary(dplyr)\r\nlibrary(ggplot2)\r\nlibrary(tidyr)\r\nlibrary(terra)\r\n\r\n# Establecer rutas a archivos importantes.\r\nruta_ie <- \"../../data/ie/ie_yucatan_2018.tif\"\r\n\r\n# Cargar funciones auxiliares.\r\nhistograma_ie <- function(df_ie){\r\n  ggplot(df_ie, aes(ie_yucatan_2018, fill = cut(ie_yucatan_2018,11))) +\r\n    geom_histogram(show.legend = FALSE, bins = 101)+\r\n    scale_fill_brewer(palette=\"RdYlGn\", direction = -1)+\r\n    xlab(\"Integridad ecosistémica\") + \r\n    ylab(\"Cantidad de píxeles (250m)\")\r\n}\r\n\r\nplan <- drake_plan(\r\n    raw_data = rast(ruta_ie),\r\n    \r\n    data = rast_ie %>% as.data.frame(),\r\n    \r\n    histograma = histograma_ie(data),\r\n    \r\n    ggsave(\"../figuras/histograma_yucatan_ie.png\")\r\n)\r\n\r\n\r\nplan\r\n\r\n# A tibble: 4 × 2\r\n  target         command                                       \r\n  <chr>          <expr_lst>                                    \r\n1 raw_data       rast(ruta_ie)                                 \r\n2 data           rast_ie %>% as.data.frame()                   \r\n3 histograma     histograma_ie(data)                           \r\n4 drake_target_1 ggsave(\"../figuras/histograma_yucatan_ie.png\")\r\n\r\nShow code\r\n\r\nvis_drake_graph(plan)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-31-flujos-de-trabajo/1_pipeline.png",
    "last_modified": "2022-09-13T22:33:16-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-06-resultados-miro/",
    "title": "Resultados Miro",
    "description": "Como actividad final del taller se utilizó Miro para explorar oportuunnidades de mejora.",
    "author": [
      {
        "name": "Miguel Equihua Zamora y Octavio Pérez Maqueo",
        "url": {}
      }
    ],
    "date": "2022-08-31",
    "categories": [],
    "contents": "\r\nComo actividad final del taller se utilizó Miro para explorar acciones que se sugieren para mejorar la implementación, adopción y diseminación del uso de la cuenta de condicón con base en el enfoque de redes bayesianas. Podría ser de su interés saber que posible recuperar los datos de los tableros Miro directamente en R. Las instrucciones para hacerlo están aquí.\r\nA continuación se enlistan las contribuciones reunidas.\r\n\r\n\r\n\r\n\r\n\r\n\r\nIdentificación y descripción del tablero\r\n\r\nconcepto\r\ntxt\r\nid tablero\r\nuXjVPf-z5Tg=\r\nNombre\r\nEstrategia Mex-SEEA\r\nDescripción\r\nAnálisis estratégico para fortalecer la implementación del SEEA EA en México\r\nNúm. notas\r\n51\r\n\r\nlos participantes en el ejercicio Miro fueron los que se registraron con los siguientes nombres.\r\n\r\nParticipante\r\nParticipante\r\nGSNMF CONAFOR\r\nVicente\r\nMiguel Eq\r\nMaria Marquez\r\nJosé de la Torre\r\nFEDERICO GONZALEZ\r\nAl\r\nMiguel Equihua Zamora\r\njequihua\r\nChristian Lomelín\r\nOctavio Perez Maqueo\r\nJuan Andrés Pérez Trejo\r\nCesar M-García\r\nAngelica Cibrian\r\nIleana Aldrete Leal\r\nSandra Medina\r\nJason Aranda\r\nDiana\r\nMiguel Angel Ramírez Martínez\r\nvictor salazar\r\nUlises Esparza\r\nJose Guevara\r\nhumberto Ramos\r\nMontserrath\r\n\r\n\r\n\r\n\r\nFigure 1: caption\r\n\r\n\r\n\r\nAspectos analizados\r\nEl análisis consideró cuatro aspectos.\r\n\r\nAspecto\r\nActuar ¿Qué deberíamos hacer en seguida?\r\nContinuar ¿Qué nos está funcionando bien?\r\nInventar ¿Hay una mejor manera de hacer las cosas?\r\nResolver ¿Qué nos está impidiendo avanzar?\r\n\r\nContinuar\r\nEn el ámbito de lo que se estima está funcionando bien se anotó lo siguiente.\r\n\r\nIdea\r\nLa sistematización del proceso de generación del indicador\r\nUtilizar la información disponible en el país, que sirve para todos\r\nEl uso de información oficial\r\nEl trabajo multidisciplinario\r\nTrabajo interinstitucional para la definición de objetivos y líneas de acción\r\nLa homologación en el lenguaje en común con todos los interesados. También el entendimiento y apropiación de la metodología.\r\nLa generación de diálogo entre los expertos para formar acuerdos y búsqueda de áreas de oportunidad.\r\nAprovechar el impulso internacional para el proyecto de cuentas ecosistémicas\r\nCon la definición/identificación de las variables explicativasHumberto Ramos\r\n\r\nResolver\r\nSe identificaron algunos asuntos que se considera no están adecuadamente resueltos o aspectos que están obstaculizando actualmente el desarrollo y adopción del enfoque propuesto para la integración de la cuenta de condición.\r\n\r\nIdea\r\nNo se garantiza la producción sistemática de información. Es necesario conocer cual es el futuro de fuentes de información como el InFyS y la adquisisión de información satelital\r\nLimitantes en el manejo de software específico para el análisis.\r\nIncluir dentro de la clasificación los usos de suelo como la minería y la ganadería\r\nEl uso de herramientas (software) libre o de uso más generalizado, ya que las faltas de licencias puede ser un obstáculo para la actualización, mejora y ampliación del Índice\r\nInformación georreferenciada de actividad agrícola y ganadera\r\nDefinir una clasificación de los ecosistemas que pueda interoperar con diferentes proyectos (Cuentas de Bosques, Cuentas Urbanas, Cuentas de Océanos)\r\nLa limitación en la información o incertidumbre sobre su actualización\r\nLa falta de un comité o grupo formal para generar y asentar acuerdos.\r\nContinuidad en la generación de informaciónen campo como el INFyS\r\nLa determinación de las responsabilidades por cada Institución.\r\nActualización del índice para otros años de estudios. Idealmente que sean para los mismos años de las CUSV del INEGI\r\nGarantizar que cuenten con las licencias y equipos de cómputo con la capacidad de realizarlo.\r\nIncluir las actividades relacionadas en los programas anuales de trabajo.\r\nEn caso de no tener capacitación en uso de software especializado, apoyar para su utilización(Capacitación)\r\nRealizar capacitaciones en linea de los temas presentados\r\nInformación Georreferenciada de actividades de minería en Áreas Naturales ProtegidasVíctor Salazar\r\nGenerar un modelo multiescala, que considere variables específicas por zona/escalaHumberto Ramos\r\nFalta de Salida al campo por reducción de presupuestoVíctor Salazar\r\nla manera de integrar en trabajo multidisciplinario a las distintas instituciones, incluida la capacitaciónHumberto Ramos\r\nFalta de Salida al campo por reducción de presupuestoVíctor Salazar\r\n\r\nInnovar\r\nLos participantes identificaron algunos elementos que consideran aportarían innovaciones valiosas para mejorar el desarrollo de la cuenta de condición o aspectos sobre los que se debería continuar trabajando para producir un mejor resultado.\r\n\r\nIdea\r\nAún me queda duda de cómo se genera la homologación del indicador a nivel mundial\r\nTomar en cuenta la retroalimentación de otras instituciones al intentar replicar el ejercicio realizado.\r\nIntegrar bases de datos de información económica para la valoración de los Servicios de los Ecosistemas\r\nAjustar la información de asentamientos humanos con información de ciudades sostenibles e infraestructura verde-azul\r\nLa sistematización podría favorecer su desarrollo y aprovechamiento.\r\nIntegrando otras variables ecológicas y económicas, así como adoptar una clasificación acordada de los ecosistemas\r\n¿Será posible realizar un ejercicio similar para ecosistemas acuáticos?\r\nUn Pipeline que integre a los datos de distintas institucionesHuumberto Ramos\r\n\r\nActuar\r\nFinalmente, se hicieron algunas sugerencias de acciones de puesta en práctica inmediata que se estima mejorarían ampliamente el desarrollo, implementación y adopción general de la cuenta de condición del Sistema de Contabilidad de los Ecosistemas.\r\n\r\nIdea\r\nProfundizar en las redes bayesianas, como parte del flujo de trabajoHumberto Ramos\r\nComité Técnico Especializado con subgrupos de trabajo\r\nSocializar metodologías para las demás instituciones con una pequeña capacitación.(Áreas de cada institución familiarizadas con este tipo de análisis)\r\nForo Técnico: Identificación de proyectos en común, definir líneas de acción, priorizar líneas de acción para interoperar en el corto, mediano y largo plazo.\r\nSocializar el blog desarrollado en el curo con la comunidad ambiental (CONAGUA, SEMARNAT. ACADËMICOS, etc.) Angélica Hernández\r\nProducción automatizada de productos para optimizar recursos institucionales en INEGI\r\nPoner en práctica lo visto en el taller para la generación de más dudas. Esas mismas compartirlas con el grupo de trabajo por si es una duda en común.\r\nDebatir sobre las áreas de oportunidad o uso potencial para los diferentes sectores.\r\nRealizar una memoria de cálculo a detalle sobre el proceso de la generación del índice y que esté disponible para todos para que exista una sola versión\r\nCreación de un grupo de trabajo dentro del CTE en el que este en constante discusión la metodología del Índice\r\nGenerar:Memoria/guion del proceso de ojo de expertosMemoria de cálculo del proceso en general\r\nDeterminar los puntos focales por Institución\r\nRealizar ejercicios de Redes BayesianasSandra Medina\r\nRealizar un proyecto compartiendo procesos en equiposSandra Medina\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-06-resultados-miro/../../figuras/Miro_lleno.png",
    "last_modified": "2022-09-13T22:43:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-28-prepara-datos-para-modelar/",
    "title": "Prepara datos para modelar",
    "description": "Procedimiento de preparción de datos a partir de GeoTiffs para producir datos tabulares\nmás adecuados para el análisis estadístico.",
    "author": [
      {
        "name": "Miguel Equihua Zamora",
        "url": {}
      }
    ],
    "date": "2022-08-30",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nAunque este proceso puede hacerse con la nueva biblioteca terra. Se dice que la biblioteca raster es más eficiente y conveniente para hacer la tarea que nos proponemos resolver, sin embargo, terra está evolucionando y podría ya no ser así. Lo que haremos aquí es leer un montón de archivos raster en formato GeoTiff. Generaremos una colección congruente con ellos en términos de que comparten proyección y extensión y luego haremos una transformación de los objetos de datos para producir una tabla convencional de datos.\r\nLocalización de los archivos\r\nEn la seción de ayer usamos la biblioteca tools y de ella la función list_files_with_exts. Volvamos a hacerlo.\r\n\r\n\r\nlibrary(\"tools\")\r\nmapas_dir <- \"../../../data/indep_vars\"\r\n\r\n# Lista de los rasters considerados variables explicativas.\r\nlst_gtif <- list_files_with_exts(mapas_dir, exts = \"tif\")\r\nlst_gtif\r\n\r\n [1] \"../../../data/indep_vars/AlturaFusteLimpio_desvest_250m.tif\"\r\n [2] \"../../../data/indep_vars/AlturaFusteLimpio_media_250m.tif\"  \r\n [3] \"../../../data/indep_vars/AlturaTotal_desvest_250m.tif\"      \r\n [4] \"../../../data/indep_vars/AlturaTotal_media_250m.tif\"        \r\n [5] \"../../../data/indep_vars/DiametroCopa_desvest_250m.tif\"     \r\n [6] \"../../../data/indep_vars/DiametroCopa_media_250m.tif\"       \r\n [7] \"../../../data/indep_vars/DiametroNormal_desvest_250m.tif\"   \r\n [8] \"../../../data/indep_vars/DiametroNormal_media_250m.tif\"     \r\n [9] \"../../../data/indep_vars/hemerobia_250m.tif\"                \r\n[10] \"../../../data/indep_vars/net_photo_mean.tif\"                \r\n[11] \"../../../data/indep_vars/net_photo_mean_lluvias.tif\"        \r\n[12] \"../../../data/indep_vars/net_photo_mean_secas.tif\"          \r\n[13] \"../../../data/indep_vars/net_photo_sd.tif\"                  \r\n[14] \"../../../data/indep_vars/presenciaArbolesMuertos_250m.tif\"  \r\n[15] \"../../../data/indep_vars/presenciaInsectos_250m.tif\"        \r\n[16] \"../../../data/indep_vars/propor_bosque.tif\"                 \r\n[17] \"../../../data/indep_vars/propor_pastizaloagricultura.tif\"   \r\n[18] \"../../../data/indep_vars/propor_selva.tif\"                  \r\n[19] \"../../../data/indep_vars/propor_suelodesnudo.tif\"           \r\n[20] \"../../../data/indep_vars/propor_urbano.tif\"                 \r\n[21] \"../../../data/indep_vars/srtm90m_mean250m.tif\"              \r\n[22] \"../../../data/indep_vars/srtm90m_range250m.tif\"             \r\n[23] \"../../../data/indep_vars/zvh_31_lcc_h.tif\"                  \r\n\r\nPueden ver que hemos encontrado 25 capas de información. Cada una de ellas aportará una columna de datos a nuestra tabla. Para hacer esto lo que haremos es:\r\nLeer los GeoTiff uno a la vez y agregarlo a una colección brick\r\nConvertir la colección de objectos raster a una colección de puntos *vectoriales.\r\nConvertir los puntos en una tabla en la que cada capa aporta una columna.\r\nLa tabla que queremos debería tener valores para todas las variablesNoté que rasterToPoints deja pasar algunos datos faltantes que no deberían faltar!!!, Quizás sean resultado de algunas incongruencias en la representación geográfica entre los mapas. Desde luego el tema podría requerir una valoración más cuidadosa, pero para fines prácticos se puede optar por generar una tabla en la que todas las columnas tienen datos. La función de R complete.cases()permite incorporar esta consideración en la construcción de la tabla para omitir todos los registros (casos o renglones) que no tengan valores válidos para todas las variables. Esto lo hace terra en forma predeterminada, así que en este caso también sirve para obtener un resultado comparable.\r\n\r\n\r\nlibrary(raster)\r\nlibrary(terra)\r\n\r\ntiempo_inicio <- Sys.time()\r\ndatos_brk <- brick(lst_gtif[1])         # tengo que iniciar el ladrillo, así que lo hago aquí\r\nfor (r in lst_gtif[2:length(lst_gtif)]) # como ya use este mapa, el ciclo empieza en el mapa 2\r\n{\r\n  datos_brk <- addLayer(datos_brk, raster(r))\r\n}\r\n\r\ndatos_tbl <- as.data.frame(rasterToPoints(datos_brk)) # aquí ocurre la magia: imágen a tabla!!\r\ndatos_tbl <- datos_tbl[complete.cases(datos_tbl), ]   # elimina filas con datos incompletos\r\ntiempo_fin = Sys.time()\r\ntiempo_raster <- tiempo_fin - tiempo_inicio\r\n\r\ndatos_tbl_na <- as.data.frame(rasterToPoints(datos_brk)) # tabla con todo\r\ndatos_tbl_na <- datos_tbl_na[!complete.cases(datos_tbl_na),] # tabla con casos incompletos\r\n\r\n\r\nCon fines didácticos y si no por llana curiosidad, les propongo comparar el tiempo que nos toma hacer la tarea que nos propusimos con la biblioteca raster, en contraste con el que nos toma hacerlo con la biblioteca terra.\r\n\r\n\r\nlibrary(sf)\r\n# La misma tarea de conversión de geotiffs a tabla, pero ahora con terra\r\ntiempo_inicio <- Sys.time()\r\ndatos_trr <- rast(lst_gtif)    # no necesito un for, ddirectamente procesa la lista de archivos\r\ndatos_trr_v <- as.points(datos_trr, na.rm = TRUE) # convierte en vectorial de puntos \r\n\r\ndatos_trr_sf <- st_as_sf(datos_trr_v, coords = c(\"x\", \"y\"))\r\n\r\ncoords_trr <- as.data.frame(st_coordinates(datos_trr_sf$geometry))\r\ndatos_trr_df <- as.data.frame(datos_trr_sf)\r\ndatos_trr_tbl <- cbind(coords_trr, datos_trr_df)\r\ndatos_trr_tbl <- datos_trr_tbl[, -length(datos_trr_tbl)]  # elimino la última columna (geometry)\r\ntiempo_fin = Sys.time()\r\ntiempo_terra <- tiempo_fin - tiempo_inicio\r\n\r\n\r\nComparación de tiempos\r\nBiblioteca\r\ntiempo\r\nRaster\r\n8.67980003356934\r\nTerra\r\n9.49797582626343\r\nLos procesos que aplicamos ¿producen los mismos resultados?. Podemos usar esté código para explorar la cuestión\r\n\r\n\r\nlibrary(tidyverse)\r\nnombres <- names(datos_trr_tbl) # X, Y van en minúsculas\r\nnombres[1:2] <- c(\"x\", \"y\")\r\nnames(datos_trr_tbl) <- nombres\r\nrow.names(datos_trr_tbl) <- NULL   # Elimino nombres de fila (dato que no necesito aquí)\r\nrow.names(datos_tbl) <- NULL       # Elimino nombres de fila (dato que no necesito aquí)\r\n\r\ncat(\"Las tablas raster y terra:\\n\")\r\n\r\nLas tablas raster y terra:\r\n\r\ncat(\"¿Tienen la misma estructura?\", same_src(datos_tbl, datos_trr_tbl), \"\\n\")\r\n\r\n¿Tienen la misma estructura? TRUE \r\n\r\ncat(\"¿Tienen los mismos datos?\", all.equal(datos_tbl, datos_trr_tbl), \"\\n\")\r\n\r\n¿Tienen los mismos datos? TRUE \r\n\r\nGuardar la tabla en disco\r\nAhora sólo resta guardar los datos en algun lugar de mi conveniencia. Lo haré en este caso con la biblioteca data.table que es super eficiente para datos grandes, así que conviene que la conozcan. Esta biblioteca contiene la función fwrite que es la que utilizaremos. Crearemos un archivo csv (valores-separados-por-comas). Anote las opciones que requiero para hacer el archivo csv como lo necesito. En R el símbolo “#” indica que lo que sigue la máquina no debe interpretarlo, así que puede usarse para añadir comentarios para el consumo humano.\r\n\r\n\r\nlibrary(data.table)\r\nfwrite(datos_tbl, # datos obtenidos en el proceso anterior\r\n          file = paste0(mapas_dir, \"/datos_yuc_2018.csv\"), # destino y nombre del archivo\r\n                        sep = \",\",           # La coma que separa los valores\r\n                        quote = FALSE,       # No quiero que ponga los nombres entre comillas\r\n                        na = \"*\",            # Netica utiliza * como indicador de dato faltante\r\n                        row.names = FALSE,   # No quiero una primera columna de números \"id\"\r\n                        showProgress = FALSE)# No necesito una barra de progreso\r\n\r\n\r\nCon este proceso hemos preparado una tabla de datos adecuada para ser leída en NETICA y podemos continuar con el procedimiento de construcción de los datos que utilizaremos en la contabilidad.\r\nEn Netica se necesitará un pequeño archivo de control que especifica los datos que deseamos recuperar al procesar un nuevo archivo de datos. Para simplificar las cosas lo haremos desde aquí.\r\n\r\n\r\n# Genera archivo de control para Netica\r\nlineas <- \"expval(integridad)\"  # Anota el valor esperado según la evidencia dada\r\n\r\nwrite(lineas, paste0(mapas_dir, \"/control_iie_netica.txt\"), sep = \"\\n\") # dato por línea \"\\n\"\r\n\r\n\r\nMapear resultados de Netica\r\nUna vez entrenado el modelo de red bayesiana y procesados los casos base para el mapeo, tenemos un mapa de valore predichos. Dada la especificación del archivo de control, ese archivo de salida tendrá una sola columna y tantos renglones como los que tenemos en el archivo de datos utilizaado como fuente de evidencia. El orden de los casos es también el mismo del archivo de datos, así que podemos agregar la nueva comuna del índice de integridad a la tabla que procesamos anteriormente para generar los datos.\r\n\r\n\r\nie_dato <- fread(paste0(mapas_dir, \"/salida.txt\"), header = TRUE, col.names = \"iie\")\r\ntemp <- unique(ie_dato)\r\n\r\n# Lo normalizo para expresarlo entre 0 (nula iie) y 100 (alta iie)\r\nie_dato <- 100 * (18 - ie_dato) / 18\r\n\r\ndatos_tbl_ie <- cbind(datos_tbl[,c(1:2)], ie_dato) \r\nknitr::kable(head(datos_tbl_ie), caption = \"Datos lat-lon e iie\")\r\n\r\nTable 1: Datos lat-lon e iie\r\nx\r\ny\r\niie\r\n3935625\r\n1135375\r\n15.43778\r\n3932125\r\n1135125\r\n68.02300\r\n3932375\r\n1135125\r\n80.22961\r\n3932625\r\n1135125\r\n83.39072\r\n3932875\r\n1135125\r\n78.98461\r\n3933125\r\n1135125\r\n81.42372\r\n\r\nYa tenemos una nueva tabla que incluye coordenadas lat-lon, y el nuevo dato del índice de integridad ecosistémica. Sólo resta convertir estos datos en en una tabla adecuada para representación espacial y a partir de ahí generar el GeoTiff que represente el resultado que obtuvimos.\r\n\r\n\r\nlibrary (sp)\r\ncoordinates(datos_tbl_ie) <-  ~ x + y\r\ngridded(datos_tbl_ie) <-  TRUE\r\nclass(datos_tbl_ie)\r\n\r\n[1] \"SpatialPixelsDataFrame\"\r\nattr(,\"package\")\r\n[1] \"sp\"\r\n\r\nplot(datos_tbl_ie)\r\n\r\n\r\n\r\nAhora convierto esta estructura de dato en uun objeto raster, efectivamente será ya el mapa final en memoria. El que guardaremos en disco finalmente.\r\n\r\n\r\ndatos_ie_r <- raster(datos_tbl_ie) \r\n\r\ncat(\"tamaño x, y del pixel: \", res(datos_ie_r))\r\n\r\ntamaño x, y del pixel:  250 250\r\n\r\nwriteRaster(datos_ie_r, filename=paste0(mapas_dir, \"/../ie/iie_yucatan.tif\"), \r\n            format=\"GTiff\", overwrite=TRUE)\r\n\r\nplot(datos_ie_r)\r\n\r\n\r\n\r\nHemos llegado al final del proceso y ya tenemos el mapa del índice de integridad ecosistémica en el disco.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-28-prepara-datos-para-modelar/../../figuras/Proceso_datos_redes.jpg",
    "last_modified": "2022-09-13T22:49:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-30-resultados/",
    "title": "Resultados",
    "description": "Generación de reportes dinámicos y análisis con datos embebidos. Procesos reproducibles de integración analítica de reportes para consumo humano.",
    "author": [
      {
        "name": "Miguel Equihua Zamora",
        "url": {}
      }
    ],
    "date": "2022-08-30",
    "categories": [],
    "contents": "\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"../../figuras/resultados.jpg\")\r\n\r\n\r\n\r\nR Markdown\r\nEste es un documento de R Markdown. Markdown es una sintáxis simple para editar HTMLs, PDFs y documentos de Microsoft Word. Para más detalles ver: http://rmarkdown.rstudio.com.\r\nCuando presionas el botón Knit un documento será generado que incluye tanto el texto plano que incluyas, como código HTML, código matemático LaTeX y código R. Estos pueden ser desplegados para fines didácticos o pueden ser ocultados para simplemente generar un documento sin código.\r\nSi escribo la opción **echo = FALSE** el código no es desplegado, por eso, en este caso, lo he puesto en TRUE. Hay varias opciónes para el bloque de código que permiten controlar los resultados y como se presentan al lector humano.\r\nDistribución de IE por municipio para Yucatán\r\nPor poner un ejemplo podemos cargar el mapa de IE correspondiente al estado de Yucatán utilizando la biblioteca terra.\r\n\r\n\r\nShow code\r\n\r\nlibrary(\"terra\")\r\n\r\nie_yucatan <- rast(\"../../../data/ie/ie_yucatan_2018.tif\")\r\n\r\n\r\nLuego podemos cargar un shapefile de los municipios del estado.\r\n\r\n\r\nShow code\r\n\r\nmu_yucatan <- vect(\"../../../data/shapefiles/Basemap_municipios.shp\")\r\n\r\n\r\nPero como podemos ver tienen proyecciones ligeramente distintas.\r\n\r\n\r\nShow code\r\n\r\ncrs(ie_yucatan, proj = TRUE)\r\n\r\n[1] \"+proj=lcc +lat_0=12 +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +datum=WGS84 +units=m +no_defs\"\r\n\r\nShow code\r\n\r\ncrs(mu_yucatan, proj = TRUE)\r\n\r\n[1] \"+proj=lcc +lat_0=12 +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +ellps=GRS80 +units=m +no_defs\"\r\n\r\nPodemos asignar el sistema de coordenadas de referencia del shapefile “al vuelo”.\r\n\r\n\r\nShow code\r\n\r\nmu_yucatan <- project(mu_yucatan, crs(ie_yucatan))\r\n\r\n\r\nLuego extraer los pixeles correspondientes a cada municipio.\r\n\r\n\r\nShow code\r\n\r\nmu_yucatan_ei <- extract(ie_yucatan, mu_yucatan)\r\nmu_yucatan_ei <- mu_yucatan_ei[complete.cases(mu_yucatan_ei),]\r\n\r\n\r\nPara luego, finalmente, ver la distribución de IE por municipio:\r\n\r\n\r\nShow code\r\n\r\nlibrary(\"ggplot2\")\r\nlibrary(\"ggridges\")\r\nlibrary(\"tidyverse\")\r\n\r\nmu_yucatan_ei <- right_join(mu_yucatan_ei, as.data.frame(mu_yucatan), \r\n                            by = c(\"ID\" = \"OID\"))\r\n\r\nmu_mean <- group_by(mu_yucatan_ei, NOMGEO) %>%\r\n           summarise(meanei = mean(ie_yucatan_2018, na.rm = TRUE))%>%\r\n            arrange(-meanei)\r\n\r\nmu_yucatan_ei$NOMGEO <- factor(mu_yucatan_ei$NOMGEO,\r\n                                levels = rev(mu_mean$NOMGEO))\r\n                     \r\nggplot(mu_yucatan_ei,\r\n       aes(x = ie_yucatan_2018, y = NOMGEO, fill = \"darkred\")) +\r\n  geom_density_ridges(fill = \"dark red\", alpha = 0.6) +\r\n  theme_ridges() + \r\n  xlim(0, 1)+\r\n  theme(\r\n    legend.position=\"none\",\r\n    panel.spacing = unit(0.1, \"lines\"),\r\n    strip.text.x = element_text(size = 8)) +\r\n  xlab(\"Integridad Ecosistémica\") + \r\n  ylab(\" \")\r\n\r\n\r\n\r\nReporte dinámico basado en datos\r\nHay una tendencia actual a la producción de de documentos vivos. Es el caso de este trabajo sobre procesos reproducibles de análisis. En el mismo sentido están surgiendo capacidades como las del officeverse, con el interés de vincular herramientas de office con R. Aries mismo, en el “ecosistema SEEA EA” avanza en esta misma dirección.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"../../figuras/procesos reproducible de análisis.png\")\r\n\r\n\r\n\r\n\r\nUn poco en esta misma tónica hicimos un ensayo en el piloto NCAVES. Produjimos un reporte dinámico. El código que desarrollamos para hacer esto, usando R y RMarkdown está aquí. El resultado lo encontraras en este vínculo, pero para verlo rapidamente también lo hemos incluido aquí, en el recuadro siguiente.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-30-resultados/../../figuras/resultados.jpg",
    "last_modified": "2022-09-15T23:54:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-09-14-clculo-con-netica/",
    "title": "Cálculo del Índice con Netica",
    "description": "Ejercicio que ejemplifica la obtención de los valores del índice con Netica.",
    "author": [
      {
        "name": "Octavio Pérez Maqueo y Miguel Equihua Zamora",
        "url": {}
      }
    ],
    "date": "2022-08-30",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nUso de Netica para el calculo del índice de integridad ecosistémica que se us+o para generar la cuenta de condición de los ecosistemas de México.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-09-14-clculo-con-netica/../../figuras/Netica_portada.png",
    "last_modified": "2022-09-14T01:59:32-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-26-dinmica-del-taller/",
    "title": "Dinámica del Taller",
    "description": "Objetivos y plan de trabajo.",
    "author": [
      {
        "name": "Octavio Pérez Maqueo, Miguel Equihua Zamora y Julián Equihua Benítez",
        "url": "https://ie-gamma.net"
      }
    ],
    "date": "2022-08-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nTaller sobre Cuenta de Condición\r\nLos días 29, 30 y 31 de agosto se desarrolla este taller con la participación de personal de INEGI y SEMARNAT. Los conductores del taller son:\r\nOctavio Pérez Maqueo\r\nMiguel Equihua Zamora\r\nJulián Equihua Benítez\r\nAgenda del taller\r\n\r\n\r\nTema\r\n\r\n\r\nInicia\r\n\r\n\r\nTermina\r\n\r\n\r\nDuración\r\n\r\n\r\nDía 29\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nBienvenida y presentación del taller\r\n\r\n\r\n09:00\r\n\r\n\r\n09:10\r\n\r\n\r\n00:10\r\n\r\n\r\nDinámica del taller\r\n\r\n\r\n09:10\r\n\r\n\r\n09:30\r\n\r\n\r\n00:20\r\n\r\n\r\nIntroducción\r\n\r\n\r\n09:30\r\n\r\n\r\n09:50\r\n\r\n\r\n00:20\r\n\r\n\r\nConceptos clave\r\n\r\n\r\n09:50\r\n\r\n\r\n10:20\r\n\r\n\r\n00:30\r\n\r\n\r\nDescanso\r\n\r\n\r\n10:20\r\n\r\n\r\n10:40\r\n\r\n\r\n00:20\r\n\r\n\r\nCáculo del índice de integridad\r\n\r\n\r\n10:40\r\n\r\n\r\n11:20\r\n\r\n\r\n00:40\r\n\r\n\r\nIntegración de información\r\n\r\n\r\n11:20\r\n\r\n\r\n14:20\r\n\r\n\r\n03:00\r\n\r\n\r\nDía 30\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nPrepara datos para modelar\r\n\r\n\r\n10:00\r\n\r\n\r\n11:30\r\n\r\n\r\n01:30\r\n\r\n\r\nCálculo del índice en Nética\r\n\r\n\r\n11:30\r\n\r\n\r\n13:00\r\n\r\n\r\n01:30\r\n\r\n\r\nDescanso\r\n\r\n\r\n13:00\r\n\r\n\r\n13:30\r\n\r\n\r\n00:30\r\n\r\n\r\nPresentación de resultados\r\n\r\n\r\n13:30\r\n\r\n\r\n14:30\r\n\r\n\r\n01:00\r\n\r\n\r\nInteroperabilidad\r\n\r\n\r\n14:30\r\n\r\n\r\n15:00\r\n\r\n\r\n00:30\r\n\r\n\r\nDía 31\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nCreación flujo de trabajo\r\n\r\n\r\n10:00\r\n\r\n\r\n11:00\r\n\r\n\r\n01:00\r\n\r\n\r\nDiscusión por grupos\r\n\r\n\r\n11:00\r\n\r\n\r\n12:00\r\n\r\n\r\n01:00\r\n\r\n\r\nDescanso\r\n\r\n\r\n12:00\r\n\r\n\r\n12:20\r\n\r\n\r\n00:20\r\n\r\n\r\nSintesis: futuro del sistema y consideraciones estratégicas\r\n\r\n\r\n12:20\r\n\r\n\r\n13:20\r\n\r\n\r\n01:00\r\n\r\n\r\nPreguntas y respuestas\r\nHemos preparado el taller casi por completo en el repositorio Taller_cond_2022 en Github. Proponemos dar seguimiento a dudas puntuales a través de la opción de issues. Es además un espacio de aprendizaje, pues las respuestas a las preguntas que se vayan formulando quedarán almacenadas aquí y podrán ser consultadas cuando sea necesario. Para hacer una consulta nueva simplemente habrá que oprimir el botón de new issue e ingresar la consulta en forma suficientemente detallada (quizás con algún ejemplo) y dar seguimiento a la discusión que la acompañará hasta que se declare resuelto el asunto. Las pantallas respectivas se ilustran a continuación.\r\n\r\nConsideraciones estratégicas\r\nPara reflexionar sobre aspectos estratégicos de la gestión del sistema de cuentas ambientales basadas en ecosistemas (SEEA EA), proponemos utilizar este pizarra virtual en Miro. Les solicitamos que vayan anotando ideas ahí, conforme les vayan surgiendo. Tenemos previsto tener una sesión de discusión sobre la perspectiva futura del sistema el Miércoles 1. Lo haremos con base en esta pizarra virtual.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-26-dinmica-del-taller/../../figuras/Portada-CME-30.png",
    "last_modified": "2022-09-15T16:58:35-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-26-introduccin/",
    "title": "Introducción",
    "description": "Se presenta el marco conceptual general del nuevo sistema de contabilidad ambiental\nSEEA EA y su contexto nacional e internacional de desarrollo.",
    "author": [
      {
        "name": "Francisco Guillén Martín",
        "url": "https://https://www.inegi.org.mx/contenidos/eventos/2013/fne/S_Francisco_Guillen.pdf"
      }
    ],
    "date": "2022-08-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nMi carrera profesional se ha desarrollado principalmente en el sector público, en el que he colaborado por más de 30 años. Particularmente he trabajado para el Instituto Nacional de Estadística y Geografía (INEGI). Hace años tuve la oportunidad de dar a conocer la primera serie anual del Producto Interno Bruto por Entidad Federativa. Fui iniciador y coordinador del proyecto denominado Sistema de Cuentas Económicas y Ecológicas de México (SCEEM). Más recientemente, fui iniciador y coordinador del nuevo proyecto denominado Sistema de Cuentas Económicas y Ecológicas de México (SCEEM), llevado a cabo por el INEGI y auspiciado por la Organización de las Naciones Unidas y el Banco Mundial, proyecto pionero en su tipo a nivel mundial.\r\nParticipo en el Buro de Expertos de Cuentas Nacionales y Financieras de la OCDE. También he participado en varios grupos de expertos asociados a la economía y el medio ambiente. Actualmente soy Director General Adjunto de Cuentas Nacionales del INEGI, así que tengo bajo mi responsabilidad el Sistema de Cuentas Nacionales de México en su conjunto. Les hablaré sobe el Sistema de Cuentas de los Ecosistemas de México.\r\n\r\nEl participante comprende el marco conceptual del SEEA EA y su relevancia en el marco de beyond GDP y la agenda de desarrollo sostenible.\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-26-introduccin/../../figuras/Francisco-guillen.png",
    "last_modified": "2022-09-16T00:26:45-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-27-conceptos-clave/",
    "title": "Conceptos clave",
    "description": "Se describen los principales conceptos y definiciones operativas usadas en el\nsistema de contabilidad ambiental basada en ecosistemas (SEEA EA)",
    "author": [
      {
        "name": "Octavio Pérez Maqueo",
        "url": {}
      }
    ],
    "date": "2022-08-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nEn esta presentación buscamos compartir estructura conceptual y las definiciones operativas de los elementos que intervienen la integración de un sistema de contabilidad ambiental y económica basado en ecosistemas (SCAE CE, SEEA EA en inglés). El objetivo que proponemos para esta participación es:\r\n\r\nEl participante identifica los conceptos clave de la tipología SCAE EA y conoce la forma como se aplicó el enfoque en el piloto mexicano\r\n\r\nLo que sigue es el vínculo a la presentación que usaremos para discutir el tema.\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-27-conceptos-clave/../../figuras/sp-Enfoque espacial_es.png",
    "last_modified": "2022-09-16T00:19:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-27-integracin-de-datos/",
    "title": "Integración de datos",
    "description": "Una introducción a la integración de datos espaciales para el modelado de IE.",
    "author": [
      {
        "name": "Julián Equihua",
        "url": "https://taller-cuenta-condicion.netlify.app/posts/2022-08-27-integracin-de-datos/"
      }
    ],
    "date": "2022-08-29",
    "categories": [],
    "contents": "\r\nAprendizaje de máquina (Machine learning)\r\nMétodos computacionales para aprender de datos con el fin de replicar una tarea o toma de decisión.\r\nExisten varios paradigmas en aprendizaje de máquina, con distintos objetivos:\r\nEl aprendizaje supervisado procura predecir o estimar una variable respuesta a partir de datos de entrada\r\nEl apprendizaje no supervisado procura describir estructuras interesantes en datos, donde no necesariamente hay una respuesta que predecir\r\nEl aprendizaje semi-supervisado procura utilizar datos asociados a una variable respuesta al mismo tiempo que datos sin la variable para procurar mejor estimarla. Estos es intenta explotar datos sin etiquetas para mejorar la estimación.\r\nEl aprendizaje por refuerzo intenta estimar la mejor política posible en un problema de toma de decisiones secuenciales.\r\nEjemplos de tareas de aprendizaje:\r\nPredecir si un individuo va a sobrevivir el hundimiento del Titanic,\r\nDetectar llamados de murciélagos en grabaciones ultrasónicas,\r\nClasificar especies de animales en imágenes digitales producidas a través de foto trampeo,\r\nEstimar el ingreso mensual de un hogar a partir de las características de la vivienda,\r\nDividir a los clientes de Netflix según sus gustos.\r\nRecomendar artículos a clientes de un programa de lealtad o servicio online.\r\n Ejercicio 1: De los ejemplos anteriores, cuáles consideran que corresponden a aprendizaje supervisado, cuáles a aprendizaje no supervisado y cuáles se podrían resolver con ambos?\r\nLas razones usuales para intentar resolver estos problemas computacionalmente son diversas:\r\nQuisiéramos obtener una respuesta barata, rápida, automatizada, y con suficiente exactitud y precisión. Por ejemplo, reconocer caracteres en una placa de coche de una fotografía se puede hacer por personas, pero eso es lento y costoso.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"1_license.jpeg\")\r\n\r\n\r\n\r\nIgual ver cada fotografía para detectar e identificar los animales ahí presentes toma muchísimo tiempo.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"2_trampa.jpg\")\r\n\r\n\r\n\r\nQuisiéramos superar el desempeño actual de los expertos o de reglas simples utilizando datos: por ejemplo, en la decisión de dar o no un préstamo a un solicitante, puede ser posible tomar mejores decisiones con algoritmos que con evaluaciones personales o con reglas simples que toman en cuenta el ingreso mensual, por ejemplo.\r\n¿Lo mismo para la mejor toma de decisiones en cuanto al manejo de recursos naturales?\r\nTambién es de gran interés entender de manera más completa y sistemática el comportamiento de un fenómeno, identificando variables o patrones importantes.\r\n¿Dado un conjunto de datos ecológicos podemos hacer generar modelos que nos permitan ir explorando distintos aspectos de la integridad ecosistémica para generar conocimiento?\r\nDe una vez adelantamos que el acercamiento que se sigue para generar el índice de integridad\r\npropuesto es uno de aprendizaje supervisado. Existe una variable que sirve de evidencia para el nivel de condición de los distintos ecosistemas del país y a la par se tienen variables que permiten estimar y, eventualmente, explicar este nivel de condición. El único detalle importante aquí es que la evidencia es categórica, y el índice deseado es uno continuo entre 0 y 1.\r\nEl aprendizaje de máquina generalmente se lleva a cabo con ayuda de software especializado. Programas propietarios o de código abierto o comunmente utilizando lenguajes de programación. Por ejemplo R, python y Julia.\r\nRecomendamos ampliamente aprender los tres en la medida de lo posible pues ofrecen distintas posibilidades y ventajas. En esta sesión trabajaremos con R.\r\nManejo de datos y modelado en R\r\nPara comenzar carguemos unos datos ejemplo al espacio de trabajo de R.\r\n\r\n\r\nShow code\r\n\r\n# Cargar paquetes.\r\nlibrary(\"raster\")\r\n\r\n# Cargar un geotiff a R, de hecho el que sirve como evidencia para el modelo de IE.\r\nruta_evidencia <- \"../../../data/delta_vp/hemerobia_250m.tif\"\r\nraster_evidencia <- raster(ruta_evidencia)\r\n\r\nraster_evidencia\r\n\r\nclass      : RasterLayer \r\ndimensions : 911, 1185, 1079535  (nrow, ncol, ncell)\r\nresolution : 250, 250  (x, y)\r\nextent     : 3697500, 3993750, 908000, 1135750  (xmin, xmax, ymin, ymax)\r\ncrs        : +proj=lcc +lat_0=12 +lon_0=-102 +lat_1=17.5 +lat_2=29.5 +x_0=2500000 +y_0=0 +datum=WGS84 +units=m +no_defs \r\nsource     : hemerobia_250m.tif \r\nnames      : hemerobia_250m \r\nvalues     : 0, 18  (min, max)\r\n\r\nPara aquellos que no han experimentado con lenguajes de programación para manejo y análisis de datos, acá están pasando varias cosas. Repasamos algunas de ellas.\r\nPrimero que nada, un elemento fundamental que debemos mencionar es el de una función. Una función recibe argumentos y lleva a cabo una cierta tarea (aunque suene un poco raro también puede recibir ningún argumento y llevar a cabo una tarea). Las funciones siempre tienen un nombre y los argumentos que reciben se colocan dentro de paréntesis.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"3_funciones.png\")\r\n\r\n\r\n\r\nPor ejemplo la función library() lleva a cabo la tarea de cargar un determinado paquete de R. Un argumento que DEBE recibir es el nombre del paquete a cargar. La función raster carga un raster en el espacio de trabajo de R (en la memoria RAM) y un argumento que DEBE recibir es la ruta en el disco duro del raster que se quiere cargar.\r\nCuando una función carga algo en memoria, esto se debe asignar a un objeto. Es decir que se debe de elegir un “nombre” con el cuál referirse a lo que se está cargando. Se puede elegir cualquier nombre pero se recomienda usar unos juiciosos para recordar de qué se tratan. Para asignar en R comúnmente se usa una flechita pero también puede usarse el símbolo de igual como en otros lenguajes de programación.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"4_asigobjeto.png\")\r\n\r\n\r\n\r\nHemos llamado ‘raster_evidencia’ al que acabamos de cargar, con este objeto podemos ver una versión cruda del mismo dentro de R.\r\n\r\n\r\nShow code\r\n\r\nplot(raster_evidencia)\r\n\r\n\r\n\r\nAhora bien, un raster es una estructura que puede resultar muy útil para ciertos propósitos, pero para otros no tanto. Por ejemplo para llevar a cabo un proceso de análisis exploratorio de datos, el formato más común es el de una tabla de datos. De hecho R tiene un estándar de datos ampliamente aceptado llamado tidy data donde se analiza y describe cómo deberían de ser las tablas de datos para análisis. Dejando eso de lado, generalmente existen funciones en R (y en los otros lenguajes) para pasar de un tipo de objeto (dato) a otro. Por ejemplo de un raster a una tabla de datos:\r\n\r\n\r\nShow code\r\n\r\ntabla_evidencia <- as.data.frame(rasterToPoints(raster_evidencia))\r\n\r\nhead(tabla_evidencia)\r\n\r\n        x       y hemerobia_250m\r\n1 3939125 1135625             16\r\n2 3939375 1135625             16\r\n3 3939625 1135625             16\r\n4 3939875 1135625             16\r\n5 3940125 1135625             16\r\n6 3940375 1135625             16\r\n\r\nAcá cada fila es una observación (un píxel) y cada columna una variable coordenada x, coordenada y, el valor de hemerobia del píxel.\r\n\r\n\r\nShow code\r\n\r\nlibrary(\"ggplot2\")\r\n\r\nggplot(tabla_evidencia, aes(hemerobia_250m, fill = cut(hemerobia_250m, 11))) +\r\n  geom_histogram(show.legend = FALSE) +\r\n  scale_fill_brewer(palette=\"Greens\")\r\n\r\n\r\n\r\n Ejercicio 2: cambiar los colores de las barras del histograma para que tengan secuencia de semáforo y que verde sea lo mejor conservado y rojo lo peor.\r\nPara nuestros propósitos conviene generar una tabla en formato tidy data que contenga cada raster que tenemos, el que acabamos de cargar (la respuesta) y los que corresponden a las variables explicativas.\r\nSe debe tener en mente que todas las funciones que existen en R han sido escritas por alguien. Así que estamos en la posibilidad de escribir nuestra propia función para lograr una tarea que nos sea útil.\r\nEscribir una función nueva en R es relativamente sencillo, prácticamente basta con darle un nombre, enlistar qué parámetros recibe, y qué hace la función con estos. Por ejemplo podemos escribir la función “elevar al cuadrado” que toma un número y lo multiplica por sí mismo.\r\n\r\n\r\nShow code\r\n\r\nelevar_al_cuadrado <- function(numero){\r\n  cuadrado <- numero*numero\r\n  return(cuadrado)\r\n}\r\n\r\n\r\nYa que la función está definida y la hemos corrido, está cargada en el espacio de trabajo y podemos usarla.\r\n\r\n\r\nShow code\r\n\r\nelevar_al_cuadrado(123)\r\n\r\n[1] 15129\r\n\r\nAhora debemos pensar qué requerimos para generar la tabla de datos antes mencionada, como todo en la vida hay varias maneras de resolver esto. Una posibilidad es generar una lista de los rasters, leerlos uno por uno para luego agregarlos a un raster multi-capa o multi-banda y convertir este último en una tabla. Ahora bien, el único paso algo complicado podría ser leer los rasters uno por uno, puesto que debemos de escribir un pedazo de código que itere sobre la lista de rasters. Hagamos esto paso por paso. Primero generemos una lista de los rasters (.tif) en una ruta de interés.\r\nUsaremos primero una función del paquete ‘tools’ para enlistar todos los archivos en una\r\ncierta ruta que tengan una extensión (.tif).\r\n\r\n\r\nShow code\r\n\r\nlibrary(\"tools\")\r\n\r\n# Lista de los rasters considerados variables explicativas.\r\nruta_indep = list_files_with_exts(\"../../../data/indep_vars\",\r\n                                  exts = \"tif\")\r\n\r\nruta_indep\r\n\r\n [1] \"../../../data/indep_vars/AlturaFusteLimpio_desvest_250m.tif\"\r\n [2] \"../../../data/indep_vars/AlturaFusteLimpio_media_250m.tif\"  \r\n [3] \"../../../data/indep_vars/AlturaTotal_desvest_250m.tif\"      \r\n [4] \"../../../data/indep_vars/AlturaTotal_media_250m.tif\"        \r\n [5] \"../../../data/indep_vars/DiametroCopa_desvest_250m.tif\"     \r\n [6] \"../../../data/indep_vars/DiametroCopa_media_250m.tif\"       \r\n [7] \"../../../data/indep_vars/DiametroNormal_desvest_250m.tif\"   \r\n [8] \"../../../data/indep_vars/DiametroNormal_media_250m.tif\"     \r\n [9] \"../../../data/indep_vars/hemerobia_250m.tif\"                \r\n[10] \"../../../data/indep_vars/net_photo_mean.tif\"                \r\n[11] \"../../../data/indep_vars/net_photo_mean_lluvias.tif\"        \r\n[12] \"../../../data/indep_vars/net_photo_mean_secas.tif\"          \r\n[13] \"../../../data/indep_vars/net_photo_sd.tif\"                  \r\n[14] \"../../../data/indep_vars/presenciaArbolesMuertos_250m.tif\"  \r\n[15] \"../../../data/indep_vars/presenciaInsectos_250m.tif\"        \r\n[16] \"../../../data/indep_vars/propor_bosque.tif\"                 \r\n[17] \"../../../data/indep_vars/propor_pastizaloagricultura.tif\"   \r\n[18] \"../../../data/indep_vars/propor_selva.tif\"                  \r\n[19] \"../../../data/indep_vars/propor_suelodesnudo.tif\"           \r\n[20] \"../../../data/indep_vars/propor_urbano.tif\"                 \r\n[21] \"../../../data/indep_vars/srtm90m_mean250m.tif\"              \r\n[22] \"../../../data/indep_vars/srtm90m_range250m.tif\"             \r\n[23] \"../../../data/indep_vars/zvh_31_lcc_h.tif\"                  \r\n\r\nHay muchos tipos de objetos en R (y en todos los lenguajes de programación). Listas, arreglos, matrices, data.frames.\r\nHay distintas maneras de “recorrer” cada uno de estos objetos pero ver esto a profundidad escapa el alcance de esta sesión. Vamos a ver cómo recorrer una lista. Estas son estructuras lineales, esto es contienen objetos (¡DE CUALQUIER TIPO!) en una única dimensión y estos se pueden seleccionar dando la posición que ocupan en la lista. Como ejemplo la lista de las rutas a rasters es una con longitud 22.\r\nDe aquí que podamos, por ejemplo, elegir la ruta del décimo raster.\r\n\r\n\r\nShow code\r\n\r\nruta_indep[10]\r\n\r\n[1] \"../../../data/indep_vars/net_photo_mean.tif\"\r\n\r\nO incluso elegir sólo del 10 al 15.\r\n\r\n\r\nShow code\r\n\r\nruta_indep[10:15]\r\n\r\n[1] \"../../../data/indep_vars/net_photo_mean.tif\"              \r\n[2] \"../../../data/indep_vars/net_photo_mean_lluvias.tif\"      \r\n[3] \"../../../data/indep_vars/net_photo_mean_secas.tif\"        \r\n[4] \"../../../data/indep_vars/net_photo_sd.tif\"                \r\n[5] \"../../../data/indep_vars/presenciaArbolesMuertos_250m.tif\"\r\n[6] \"../../../data/indep_vars/presenciaInsectos_250m.tif\"      \r\n\r\nLo que haremos con esto es leer los rasters uno por uno utilizando un bucle. Los bucles (for loops) son de los paradigmas de iteración más comunes y viejos que hay. De nuevo hay muchas maneras de resolver esto. Si bien no es de las maneras más elegantes de lograr esto, es de utilidad puesto que siempre será posible utilizarlo, en cualquier lenguaje de programación, por lo universal que es.\r\nPrimero un bucle sencillo. Le diremos a la computadora, quiero que cuentes del 1 al 10 y en cada número digas la palabra ‘número’:\r\n\r\n\r\nShow code\r\n\r\nfor (i in 1:10){ # Esto se lee para i que tomará los valores 1,2,3,...,10\r\n  print(\"numero\") # Despliega el texto numero\r\n}\r\n\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n[1] \"numero\"\r\n\r\nPero también le podríamos decir, despliega el número en el que vas:\r\n\r\n\r\nShow code\r\n\r\nfor (i in 1:10){ # Esto se lee para i que tomará los valores 1,2,3,...,10\r\n  print(i) # Despliega el número en el que vas\r\n}\r\n\r\n[1] 1\r\n[1] 2\r\n[1] 3\r\n[1] 4\r\n[1] 5\r\n[1] 6\r\n[1] 7\r\n[1] 8\r\n[1] 9\r\n[1] 10\r\n\r\n Ejercicio 3: escribir un bucle que lea toda nuestra lista de rasters, uno por uno.\r\nJuntando todo lo anterior podemos escribir una función que nos permita leer todos los rasters y luego con esto generar una tabla:\r\n\r\n\r\nShow code\r\n\r\nrutasAMulti = function(dep_path,\r\n                   indep_paths){\r\n  \r\n  bnbrik = brick()\r\n  bnbrik = addLayer(bnbrik,raster(dep_path))\r\n  for (i in 1:length(indep_paths)){\r\n    bnbrik = addLayer(bnbrik,raster(indep_paths[i]))\r\n  }\r\n  return(bnbrik)\r\n}\r\n\r\n# Hemerobia etá también en ruta_indep, así que lo elimino para no duplicar la capa\r\nruta_indep <- ruta_indep[!grepl(\"hemerobia\", ruta_indep)]\r\n\r\nraster_multi <- rutasAMulti(\"../../../data/delta_vp/hemerobia_250m.tif\", ruta_indep)\r\n\r\ntabla_multi <- as.data.frame(rasterToPoints(raster_multi))\r\n\r\nhead(tabla_multi)\r\n\r\n        x       y hemerobia_250m AlturaFusteLimpio_desvest_250m\r\n1 3939125 1135625             16                     0.00000000\r\n2 3939375 1135625             16                     0.54562950\r\n3 3939625 1135625             16                     0.02781332\r\n4 3939875 1135625             16                     0.02781332\r\n5 3940125 1135625             16                     0.02781332\r\n6 3940375 1135625             16                     0.02781332\r\n  AlturaFusteLimpio_media_250m AlturaTotal_desvest_250m\r\n1                   0.00000000              0.000000000\r\n2                   1.16930819              0.693956196\r\n3                   0.03184322              0.008685708\r\n4                   0.03184322              0.008685708\r\n5                   0.03184322              0.008685708\r\n6                   0.03184322              0.008685708\r\n  AlturaTotal_media_250m DiametroCopa_desvest_250m\r\n1              0.0000000                0.02296904\r\n2              4.6368446                0.17380011\r\n3              0.0426484                0.07168859\r\n4              0.0426484                0.07168859\r\n5              0.0426484                0.07168859\r\n6              0.0426484                0.07168859\r\n  DiametroCopa_media_250m DiametroNormal_desvest_250m\r\n1              0.02391556                  0.00000000\r\n2              0.19083574                  0.78119290\r\n3              0.01392496                  0.03274944\r\n4              0.01392496                  0.03274944\r\n5              0.01392496                  0.03274944\r\n6              0.01392496                  0.03274944\r\n  DiametroNormal_media_250m net_photo_mean net_photo_mean_lluvias\r\n1                0.00000000              0                      0\r\n2                1.10098600              0                      0\r\n3                0.07778671              0                      0\r\n4                0.07778671              0                      0\r\n5                0.07778671              0                      0\r\n6                0.07778671              0                      0\r\n  net_photo_mean_secas net_photo_sd presenciaArbolesMuertos_250m\r\n1                    0            0                           NA\r\n2                    0            0                           NA\r\n3                    0            0                           NA\r\n4                    0            0                           NA\r\n5                    0            0                           NA\r\n6                    0            0                           NA\r\n  presenciaInsectos_250m propor_bosque propor_pastizaloagricultura\r\n1                     NA             0                           0\r\n2                     NA             0                           0\r\n3                     NA             0                           0\r\n4                     NA             0                           0\r\n5                     NA             0                           0\r\n6                     NA             0                           0\r\n  propor_selva propor_suelodesnudo propor_urbano srtm90m_mean250m\r\n1            0                   0      2.469136              2.0\r\n2            0                   0      0.000000              3.5\r\n3            0                   0      0.000000               NA\r\n4            0                   0      2.469136               NA\r\n5            0                   0      2.222222               NA\r\n6            0                   0      0.000000               NA\r\n  srtm90m_range250m zvh_31_lcc_h\r\n1                 0           15\r\n2                 1           15\r\n3                NA           15\r\n4                NA           15\r\n5                NA           15\r\n6                NA           15\r\n\r\nAhora podemos ajustar un primer modelo entre las variables explicativas y nuestra respuesta. Ajustaremos un modelo lineal. Por un momento dejaremos de lado la variable “zonas de vida de holdridge” [22] y también excluiremos las columnas-coordenadas.\r\n\r\n\r\nShow code\r\n\r\ndatos_modelo <- tabla_multi[,3:24] # Quitar coordenadas y ZVH.\r\n\r\ndatos_modelo <- datos_modelo[complete.cases(datos_modelo),] # Quitar casos con valores faltantes.\r\n\r\nmodelo_lineal <- lm(hemerobia_250m~., data = datos_modelo) # Ajuste modelo. \r\n# La fórmula variable~. significa variable explicada por \"todo lo demás\".\r\n\r\nsummary(modelo_lineal)\r\n\r\n\r\nCall:\r\nlm(formula = hemerobia_250m ~ ., data = datos_modelo)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-17.5742  -1.5425  -0.6471   1.1071  12.5245 \r\n\r\nCoefficients:\r\n                                 Estimate Std. Error t value Pr(>|t|)\r\n(Intercept)                     3.9607597  0.0747690  52.973  < 2e-16\r\nAlturaFusteLimpio_desvest_250m -0.6420698  0.0331591 -19.363  < 2e-16\r\nAlturaFusteLimpio_media_250m   -0.2972919  0.0129057 -23.036  < 2e-16\r\nAlturaTotal_desvest_250m        1.5600010  0.0235607  66.212  < 2e-16\r\nAlturaTotal_media_250m          0.3146922  0.0079551  39.559  < 2e-16\r\nDiametroCopa_desvest_250m       1.7902153  0.0461978  38.751  < 2e-16\r\nDiametroCopa_media_250m        -0.1151313  0.0156566  -7.354 1.93e-13\r\nDiametroNormal_desvest_250m    -0.1315326  0.0052809 -24.907  < 2e-16\r\nDiametroNormal_media_250m      -0.1055519  0.0040341 -26.165  < 2e-16\r\nnet_photo_mean                 -0.0457011  0.0022202 -20.584  < 2e-16\r\nnet_photo_mean_lluvias          0.0291710  0.0010958  26.621  < 2e-16\r\nnet_photo_mean_secas            0.0115336  0.0011334  10.176  < 2e-16\r\nnet_photo_sd                   -0.0029836  0.0002019 -14.778  < 2e-16\r\npresenciaArbolesMuertos_250m    4.0028680  0.0393549 101.712  < 2e-16\r\npresenciaInsectos_250m         -1.7508169  0.0332284 -52.690  < 2e-16\r\npropor_bosque                   1.1448459  1.5805864   0.724    0.469\r\npropor_pastizaloagricultura     0.0462580  0.0006884  67.198  < 2e-16\r\npropor_selva                    0.0124566  0.0006940  17.949  < 2e-16\r\npropor_suelodesnudo             0.0299204  0.0011090  26.980  < 2e-16\r\npropor_urbano                   0.1184421  0.0006473 182.983  < 2e-16\r\nsrtm90m_mean250m               -0.0008771  0.0001808  -4.851 1.23e-06\r\nsrtm90m_range250m              -0.0063388  0.0010698  -5.925 3.12e-09\r\n                                  \r\n(Intercept)                    ***\r\nAlturaFusteLimpio_desvest_250m ***\r\nAlturaFusteLimpio_media_250m   ***\r\nAlturaTotal_desvest_250m       ***\r\nAlturaTotal_media_250m         ***\r\nDiametroCopa_desvest_250m      ***\r\nDiametroCopa_media_250m        ***\r\nDiametroNormal_desvest_250m    ***\r\nDiametroNormal_media_250m      ***\r\nnet_photo_mean                 ***\r\nnet_photo_mean_lluvias         ***\r\nnet_photo_mean_secas           ***\r\nnet_photo_sd                   ***\r\npresenciaArbolesMuertos_250m   ***\r\npresenciaInsectos_250m         ***\r\npropor_bosque                     \r\npropor_pastizaloagricultura    ***\r\npropor_selva                   ***\r\npropor_suelodesnudo            ***\r\npropor_urbano                  ***\r\nsrtm90m_mean250m               ***\r\nsrtm90m_range250m              ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 2.625 on 593781 degrees of freedom\r\nMultiple R-squared:  0.3946,    Adjusted R-squared:  0.3946 \r\nF-statistic: 1.843e+04 on 21 and 593781 DF,  p-value: < 2.2e-16\r\n\r\nAunque no es un modelo muy bueno por múltiples razones, ya nos permite contestar algunas preguntas. Por ejemplo qué pasa si ponemos un centro urbano en algún lugar?\r\n Ejercicio 4: encontrar una observación bien conservada de la tabla de datos y utilizar el modelo para estimar qué pasaría si aumenta la cantidad de clase urbana ahí. Ayuda: investigar cómo se utiliza la función predict. Las tablas se pueden recorrer igual que las listas pero naturalmente tienen dos dimensiones en vez de solo una tabla[1:2,10:22] e incluso con condiciones, e.g. tabla[tabla$hemerobia_250m == 0,]. El símbolo de pesos se refiere a elegir una columna. \r\n¿Por qué Redes Bayesianas?\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"6_bn4.png\")\r\n\r\n\r\n\r\nFigure 1:  Variables related in someway to EI.\r\n\r\n\r\n\r\nQué influencia tienen estas variables explicativas sobre lo que llamamos IE?\r\nNuestra regresión lineal diría, por ejemplo, que IE es una suma ponderada de las variables explicativas.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"7_linear.png\")\r\n\r\n\r\n\r\nFigure 2:  Modelo lineal.\r\n\r\n\r\n\r\nPero, por ejemplo, no estamos también interesados en cómo la presencia de depredadores tope influencian la captura de carbono?\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"8_linear2.png\")\r\n\r\n\r\n\r\nFigure 3:  Qué hay de las relaciones entre variables explicativas?.\r\n\r\n\r\n\r\nLas Redes Bayesianas nos dejan proponer estructuras entrecruzadas de correlación entre variables.\r\n\r\n\r\nShow code\r\n\r\nknitr::include_graphics(\"9_bnie.png\")\r\n\r\n\r\n\r\nFigure 4:  Modelo actual de EI.\r\n\r\n\r\n\r\nUn comentario sobre armonización de datos\r\nFinalmente se debe mencionar que lo trabajado anteriormente asume que los datos de entrada (los rasters) se encuentran armonizados, esto es que mantienen la misma resolución, proyección, extent, etc…\r\nEsto puede requerir procesos más o menos complejos, como reproyectar, agregar o desagregar píxeles, rasterizar archivos vectoriales, etc… pero todo se puede hacer con lo antes expuesto y con las funciones contenidas en los paquetes de GIS en R o en cualquier otro lenguaje como python. Por supuesto es buena idea tener un raster molde el cuál utilizar como referente.\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-27-integracin-de-datos/1_license.jpeg",
    "last_modified": "2022-09-13T22:36:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-08-28-calculo-de-iie-actual/",
    "title": "Calculo de IIE actual",
    "description": "Conceptos, herramientas y mecanísmo de cálculo usado para el cómputo del\níndice de integridad ecosistémica basado en redes bayesianas.",
    "author": [
      {
        "name": "Miguel Equihua Zamora",
        "url": {}
      }
    ],
    "date": "2022-08-29",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\nEn esta presentación se describen los conceptos utilizados para integrar la estrategia analítica basada en redes bayesianas que condujo al cómputo del índice de integridad ecosistémica que se empleó en el proyecto piloto NCAVES en el que participó México. Así se obtuvieron estimadores de condición para todos los ecosistemas del país en pixeles con resolución de 250m. Las fechas nominales de estos cálculos son 2004 y 2018. En esta charla nos proponemos el siguiente objetivo.\r\n\r\n\r\nEl participante conoce la metodología para el cálculo del índice de integridad ecosistémica (IIE) y las oportunidades que ofrece la aproximación empleada.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-08-28-calculo-de-iie-actual/../../figuras/datos_geoespaciales.png",
    "last_modified": "2022-09-16T00:33:05-05:00",
    "input_file": "calculo-iie-actual.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "Bienvenida al taller sobre condición ecosistémica",
    "description": "Bienvenidos a este blog que está integrado por los materiales de capacitación\ny discusión sobre el cómputo del índice de integridad ecosistémica con el enfoque\nde redes bayesianas. Este índice se utiliza en la integración de la cuenta \nde condición.",
    "author": [
      {
        "name": "Octavio Pérez Maqueo, Miguel Equihua Zamora y Julián Equihua Benítez",
        "url": "https://i-gamma.net"
      }
    ],
    "date": "2022-08-26",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-09-02T09:36:25-05:00",
    "input_file": {}
  }
]
